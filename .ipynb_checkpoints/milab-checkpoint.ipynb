{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos en Infomática Urbana: análisis y procesamiento de datos 2017 - Fing - UDELAR\n",
    "### Docentes:\n",
    "Sebastián Baña, \n",
    "Sergio Nesmachnow,\n",
    "Renzo Massobrio\n",
    "\n",
    "\n",
    "\n",
    "## Propuesta de trabajo final: \n",
    "\n",
    "### Procesamiento de datos de consumo eléctrico en computadores multicores\n",
    "\n",
    "Jonathan Muraña - 42545567\n",
    "\n",
    ">Propuesta: presentar un Notebook que incluya y describa scripts python utilizando las bibliotecas pandas y numpy, para el procesamiento y análisis de los datos de logs generados en los experimentos de caracterización del consumo eléctrico en  multicores \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivación\n",
    "Pretendo alinear los conocimientos adquiridos en el curso para potenciar el procesamiento y análisis de datos necesarios para la realización de mi tesis de maestría en informatica. Además incluir el producto final de esta tarea como material adjunto en la publicación de un artículo relacionado en el que me encuentro trabajando. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "Una de las actividades de la tesis consta de recolectar información de consumo eléctrico de una computadora (host) mientras se realiza la ejecución de una tarea (benchmark) que tiene un determinado perfil de utilización de recursos. De esta forma se busca relacionar el consumo eléctrico con el tipo de recurso que el benchmark está utilizando y la carga de trabajo. Para eso se utiliza la siguiente infraestructura: \n",
    "\n",
    "![alt text](milab_files/images/setup.png \"Monitoring setup\")\n",
    "\n",
    "\n",
    "Los benchmarks son ejecutados en la máquina host mientras un script de polling consulta el medidor de potencia y loguea la información de consumo. Hay 4 tipos de benchmak cada uno orientado a un recurso computacional en particular (CPU, memoria, disco y red). Los benchmarks se ejecutan de forma individual y combinada, y además con distintas cargas. Un experimento consiste en uno o varios tipos de benchmarks, ejecutando, en un cierto nivel de carga. Por ejemplo el experimento \"c1l37y5\" coresponde al benchmark consumidor de CPU ejecutando con una carga de 35.5% y el experimento \"c1f1l2525\" corresponde al benckmark consumidor de CPU ejecutando combinado con el bechmark consumidor de disco (file), ambos con un nivel de carga de 25%.\n",
    "\n",
    "Cada experimento se ejecuta 20 veces para la validez estadística.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuente de datos a procesar\n",
    "\n",
    "Existen dos fuentes de datos a procesar. Por un lado los logs de consumo (escritos en logging machine) y por otro los logs de inicio y fin de las ejecuciones de benchmarks (escritos en host). Comparando la fecha hora de ambas fuentes posible entonces saber cuánta energía consumió la ejecucción de un proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log de consumo eléctrico / Cleaning data / Data conversion\n",
    "Esta información se registra haciendo polling cada X segundos sobre el medidor PDU y registrando el consumo instantáneo un archivo de texto. Aquí se levantan desde arhivos y concatenan en un dataframe todos los logs\n",
    "En particular nos interesa la fecha hora y la potencia instantánea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,3,4,7,8,9,11,12,15,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/jon/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: In future versions of pandas, match will change to always return a bool indexer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              datetime P(W)\n",
      "0  2017-02-27 23:59:59   57\n",
      "1  2017-02-28 00:00:01   57\n",
      "2  2017-02-28 00:00:02   57\n",
      "3  2017-02-28 00:00:03   57\n",
      "4  2017-02-28 00:00:04   57\n"
     ]
    }
   ],
   "source": [
    "#COLLECTING\n",
    "\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "%pylab inline\n",
    "\n",
    "#obtengo una lista de los nombre de archivos\n",
    "EXE_DIR = \"milab_files/energy/\"\n",
    "list_of_file_names= next(walk(EXE_DIR))[2]\n",
    "#print (list_of_file_names)\n",
    "#dfEnergyLog = pd.read_csv(EXE_DIR + list_of_file_names[0], sep='|')\n",
    "dfEnergyLog = pd.DataFrame()\n",
    "for file_name in list_of_file_names[:]:\n",
    "    dfEnergyLogAux=pd.read_csv(EXE_DIR + file_name, sep='|')\n",
    "    dfEnergyLog = dfEnergyLog.append(dfEnergyLogAux)\n",
    "\n",
    "#CLEANING\n",
    "\n",
    "import re\n",
    "r = re.compile('([0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2})')\n",
    "#print (dfEnergyLog[['datetime','P(W)']].tail())\n",
    "#cleanDfEnergyLog = dfEnergyLog[['datetime','P(W)']][dfEnergyLog.datetime.str.match(r)]\n",
    "#print (dfEnergyLog[['datetime','P(W)']][dfEnergyLog.datetime.str.match(r).str.len()>0])\n",
    "cleanDfEnergyLog = dfEnergyLog[['datetime','P(W)']][dfEnergyLog.datetime.str.match(r).str.len()>0]\n",
    "#cleanDfEnergyLog = new dfEnergyLog[['datetime','P(W)']]\n",
    "#cleanDfEnergyLog = dfEnergyLog[['datetime','P(W)']]\n",
    "\n",
    "\n",
    "#delenting row without date format\n",
    "cleanDfEnergyLog['datetime'] =  pd.to_datetime(cleanDfEnergyLog['datetime'][:]\n",
    "                                            , format='%Y-%m-%d %H:%M:%S')\n",
    "#type(cleanDfEnergyLog['datetime'][0])\n",
    "cleanDfEnergyLog['P(W)'] = pd.to_numeric(cleanDfEnergyLog['P(W)'])\n",
    "\n",
    "print (dfEnergyLog[['datetime','P(W)']].head())\n",
    "#print (dfEnergyLog[['datetime','P(W)']].tail())\n",
    "#print type(dfEnergyLog['P(W)'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log inicio y fin de ejecuciones / proccesing / conversion / Grouping by and FIlter\n",
    "Al iniciar y finalizar una ejecucion se crea un archivo de texto indicando en el nombre: id del experimento, id de la ejecución, si es inicio o fin y fecha hora. El siguiente código levanta la información en un DF\n",
    "\n",
    "\n",
    "### Offset entre logs\n",
    "Algo a tener en cuenta es que hay que sincronizar las horas de los logs sumando la diferencia de segundos entre las horas registradas (esto es porque las horas del host y logging machine no estan sincronizadas). Tal vez es posible sincronizar de forma automática?? \n",
    "\n",
    "TIME_OFFSET = 10810\n",
    "\n",
    "#medir time (logging machine)\n",
    "#lun feb 27 16:04:55 UYT 2017\n",
    "#node71 time (host)\n",
    "#Mon Feb 27 19:05:12 UYT 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#COLLECTING\n",
    "\n",
    "from os import walk\n",
    "\n",
    "#obtengo una lista de los nombre de archivos\n",
    "EXE_DIR = \"milab_files/executions\"\n",
    "list_of_file_names= next(walk(EXE_DIR))[2]\n",
    "\n",
    "#obtengo una lista de los nombre de archivos\n",
    "dfExecutionsLog = pd.DataFrame(list_of_file_names)\n",
    "\n",
    "#print (dfExecutionsLog.head())\n",
    "\n",
    "#PROCESING\n",
    "\n",
    "dfExecutionsLog.columns = ['data']\n",
    "\n",
    "dfExecutionsLog = pd.DataFrame(dfExecutionsLog.data.str.split('_').tolist(), columns = ['cod','exp','data'])\n",
    "dfExecutionsLogAux = pd.DataFrame(dfExecutionsLog.data.str.split('.').tolist(), columns = ['datetime','inifin'])\n",
    "\n",
    "#no millis\n",
    "dfExecutionsLog['datetime'] = dfExecutionsLogAux['datetime'].str.slice(0,-4)\n",
    "dfExecutionsLog['inifin'] = dfExecutionsLogAux['inifin'] \n",
    "\n",
    "#print dfExecutionsLog.head()[['cod','exp','datetime','inifin']]\n",
    "\n",
    "#CONVERSION\n",
    "\n",
    "dfExecutionsLog['datetime'] =  pd.to_datetime(dfExecutionsLog['datetime'][:]\n",
    "                                             , format='%Y|%m|%d-%H:%M:%S')\n",
    "#print (dfExecutionsLog.head())\n",
    "\n",
    "#GROUPING\n",
    "\n",
    "dfExecutionsLogMerged = pd.merge(dfExecutionsLog, dfExecutionsLog, on='exp')[['cod_x','exp','datetime_x','inifin_x','cod_y','datetime_y','inifin_y']]\n",
    "dfExecutionsLogMergedAndFiltered = dfExecutionsLogMerged[dfExecutionsLogMerged.inifin_x=='ini'][dfExecutionsLogMerged.inifin_y=='end']\n",
    "print (dfExecutionsLogMergedAndFiltered.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating utilization level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      cod_x                exp          datetime_x inifin_x    cod_y  \\\n",
      "1    c1l100  20170228005728816 2017-02-28 00:57:28      ini   c1l100   \n",
      "5     c1l25  20170227210017249 2017-02-27 21:00:17      ini    c1l25   \n",
      "10  c1l62y5  20170227224717136 2017-02-27 22:47:17      ini  c1l62y5   \n",
      "14  c1l37y5  20170227215039514 2017-02-27 21:50:39      ini  c1l37y5   \n",
      "17    c1l75  20170227233357099 2017-02-27 23:33:57      ini    c1l75   \n",
      "\n",
      "            datetime_y inifin_y     UL  \n",
      "1  2017-02-28 00:58:31      end  100.0  \n",
      "5  2017-02-27 21:01:17      end   25.0  \n",
      "10 2017-02-27 22:48:18      end   62.5  \n",
      "14 2017-02-27 21:51:40      end   37.5  \n",
      "17 2017-02-27 23:34:58      end   75.0  \n"
     ]
    }
   ],
   "source": [
    "    \n",
    "def cod2UL(cod):\n",
    "  #print (cod)\n",
    "  #print (type(cod))\n",
    " # print (type('12y5'))\n",
    "  if '12y5' in cod:\n",
    "        #print (\"si\")\n",
    "        return 12.5\n",
    "  if \"25\" in cod:\n",
    "        return 25\n",
    "  if \"37y5\" in cod:\n",
    "        return 37.5\n",
    "  if \"50\" in cod:\n",
    "        return 50\n",
    "  if \"62y5\" in cod:\n",
    "        return 62.5\n",
    "  if \"75\" in cod:\n",
    "        return 75\n",
    "  if \"87y5\" in cod:\n",
    "        return 87.5\n",
    "  if \"100\" in cod:\n",
    "        return 100\n",
    "    \n",
    "        \n",
    "dfExecutionsLogMergedAndFiltered['UL'] =  dfExecutionsLogMergedAndFiltered.apply(lambda x:cod2UL(x.cod_x),axis = 1) \n",
    "print (dfExecutionsLogMergedAndFiltered.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarizing the offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print (cleanDfEnergyLog.head())\n",
    "\n",
    "cleanDfEnergyLog['datetimePlusOffset']  = cleanDfEnergyLog.apply(lambda x: pd.to_datetime(x.datetime)+pd.Timedelta(seconds=10810) ,axis = 1)\n",
    "\n",
    "print (cleanDfEnergyLog.head())\n",
    "#print (cleanDfEnergyLog.tail())\n",
    "#print (dfExecutionsLogMergedAndFiltered.head())\n",
    "#print (dfExecutionsLogMergedAndFiltered.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       power_average\n",
      "UL                  \n",
      "12.5      121.587226\n",
      "25.0      136.552418\n",
      "37.5      142.531893\n",
      "50.0      152.800439\n",
      "62.5      163.190844\n",
      "75.0      175.769725\n",
      "87.5      185.826720\n",
      "100.0     194.640200\n",
      "       power_average\n",
      "UL                  \n",
      "12.5        1.662569\n",
      "25.0        2.017633\n",
      "37.5        2.115192\n",
      "50.0        2.903931\n",
      "62.5        2.745200\n",
      "75.0        1.791475\n",
      "87.5        2.250113\n",
      "100.0       4.304858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "power_average = []\n",
    "power_desv = []\n",
    "for index, row in dfExecutionsLogMergedAndFiltered.iterrows():\n",
    "  cleanDfEnergyLogCropSup = cleanDfEnergyLog[row.datetime_y > cleanDfEnergyLog.datetimePlusOffset]\n",
    "  cleanDfEnergyLogCropInfAndSup = cleanDfEnergyLogCropSup[row.datetime_x < cleanDfEnergyLogCropSup.datetimePlusOffset]\n",
    "  #cleanDfEnergyLogCropInfAndSup = cleanDfEnergyLog[row.datetime_x == cleanDfEnergyLog.datetimePlusOffset]\n",
    "  #print (cleanDfEnergyLog.size)\n",
    "  #print (cleanDfEnergyLogCropSup.size)\n",
    "  #print (cleanDfEnergyLogCropInfAndSup.size)\n",
    "  #print (row.datetime_y)\n",
    "  #print (row.datetime_x)\n",
    "  \n",
    "  #print (cleanDfEnergyLogCropInfAndSup.head())\n",
    "  #print (\"END TAIL\")\n",
    "  \n",
    "  aux_mean = cleanDfEnergyLogCropInfAndSup['P(W)'].mean()      \n",
    "  aux_std = cleanDfEnergyLogCropInfAndSup['P(W)'].std() \n",
    "    \n",
    "  if aux_mean == 0:\n",
    "        print (\"there is a zero\")\n",
    "    \n",
    "    \n",
    "  #print (aux_mean)\n",
    "\n",
    "  power_average.append(aux_mean)\n",
    "  power_desv.append(aux_std)\n",
    "    \n",
    "dfExecutionsLogMergedAndFiltered['power_average'] = pd.Series(power_average).values\n",
    "#dfExecutionsLogMergedAndFiltered['power_desv'] = pd.Series(power_desv).values\n",
    "\n",
    "#print (dfExecutionsLogMergedAndFiltered[['cod_x','datetime_x','power_average','power_desv']])   \n",
    "print (dfExecutionsLogMergedAndFiltered.groupby(['UL']).mean())\n",
    "print (dfExecutionsLogMergedAndFiltered.groupby('UL').std())\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros\n",
    "\n",
    "El notebook se documentará en idioma inglés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Además\n",
    "\n",
    "* Hay experimentos complemetarios de performance de los benchmarks que buscan relacionar el tiempo de finalización  (makespan) de los procesos con respecto a la carga. De esta forma es posible realizar análisis de eficiencia enerǵetica\n",
    "* Los experimetos se realizaron para dos host, por lo tanto hay duplicación de todos los logs, y esto permite realizar un análisis comparativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "La propuesta me parece interesante ya que va a permitir manejar con calidad y transparencia el procesamiento de los datos de los experimetos de la tesis y además abre la oportunidad de realizar nuevos análisis sobre la información que enriquezcan la investigación, como pueden ser:\n",
    "\n",
    "* validadiones estadísticas utiliando bibliotecas científicas de python\n",
    "* graficar distintas presentaciones de los datos con facilidad\n",
    "* machine learning para detectar correlaciones entre las variables\n",
    "* interpolaciones utilizando bibliotecas existentes\n",
    "* agregar nueva información empírica facilmente\n",
    "* y más ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG DE ENERGIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
